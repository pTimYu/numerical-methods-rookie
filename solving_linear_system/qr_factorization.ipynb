{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e1598d9",
   "metadata": {},
   "source": [
    "## QR Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c797bf",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "QR Factorization is another important concept for the \"factorization\" of a matrix. The characteristic is that such factorization imposes no requirements on the matrix, whether you are dealing with a non-square matrix or a non-full-rank matrix (but we still need to avoid a non-full-rank matrix).\n",
    "\n",
    "It can be written as:\n",
    "\n",
    "$$\\textbf{A}=\\textbf{QR}$$\n",
    "\n",
    "Where $\\textbf{Q}$ contains the orthonormal basis, and $\\textbf{R}$ is an upper-triangular matrix. If $\\textbf{A}$ is a non-squared tall matrix ($m\\times n$ matrix, $m>n$), we can also write it in this way:\n",
    "\n",
    "$$\n",
    "\\textbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\textbf{Q}_1 & \\textbf{Q}_2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\textbf{R}_1 \\\\ 0\n",
    "\\end{bmatrix}=\\textbf{Q}_1\\textbf{R}_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119fa805",
   "metadata": {},
   "source": [
    "### Gram-Schmidt Algorithm\n",
    "\n",
    "We have already mentioned the Gram-Schmidt orthogonality in [linear algebra notes](../basic_knowledges/linear_algebra.md). And now we will use this idea to do the QR factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78c4ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def qr_gram_schmidt(A):\n",
    "    '''\n",
    "    QR Factorization via Gram-Schmidt Algorithm\n",
    "    Input:\n",
    "    Full ranked matrix A\n",
    "    Output:\n",
    "    Factorized Q and R\n",
    "    '''\n",
    "    A = np.array(A, dtype=float)\n",
    "    row = A.shape[0]\n",
    "    col = A.shape[1]\n",
    "    Q = np.array(A, dtype=float)\n",
    "    R = np.zeros((col, col))\n",
    "    for i in range(col):\n",
    "        for j in range(i):\n",
    "            R[j, i] = Q[:, j].T @ A[:, i]\n",
    "            Q[:, i] = Q[:, i] - R[j, i] * Q[:, j]\n",
    "        R[i, i] = np.linalg.norm(Q[:, i])\n",
    "        if R[i, i] == 0:\n",
    "            raise ValueError(\"Stop: A is not full column rank.\")\n",
    "        Q[:, i] = Q[:, i] / (np.linalg.norm(Q[:, i]))\n",
    "    \n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a90525",
   "metadata": {},
   "source": [
    "### Modified Gram-Schmidt Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa5de1",
   "metadata": {},
   "source": [
    "Instead of performing decompositions incrementally, we can also decompose the subsequent vectors all at once after each basis is computed.\n",
    "\n",
    "In exact arithmetic, classical and modified algorithms are equivalent. But in floating-point arithmetic, the modified algorithm will be more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d761f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_qr(A):\n",
    "    '''\n",
    "    Modified QR Factorization\n",
    "    Input:\n",
    "    Full ranked matrix A\n",
    "    Output:\n",
    "    Factorized Q and R\n",
    "    '''\n",
    "    Q = np.array(A, dtype=float)\n",
    "    row = Q.shape[0]\n",
    "    col = Q.shape[1]\n",
    "    R = np.zeros((col, col))\n",
    "    for i in range(col):\n",
    "        R[i, i] = np.linalg.norm(Q[:, i])\n",
    "        if R[i, i] == 0:\n",
    "            raise ValueError(\"Stop: A is not full column rank.\")\n",
    "        Q[:, i] = Q[:, i] / (np.linalg.norm(Q[:, i]))\n",
    "        for j in range(i + 1, col):\n",
    "            R[i, j] = Q[:, j].T @ Q[:, i]\n",
    "            Q[:, j] = Q[:, j] - R[i, j] * Q[:, i]\n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eef0db",
   "metadata": {},
   "source": [
    "### Sensitivity analysis\n",
    "\n",
    "([Reference](https://www.math.uci.edu/~ttrogdon/105A/html/Lecture23.html)) We can do some sensitivity analysis to the two methods, and we can find that analytically the error is eliminated. First of all, let's write the mathematical expression for two algorithms.\n",
    "\n",
    "* Classical Algorithm\n",
    "$$\\mathbf{v}_j = \\mathbf{x}_j - \\sum_{i=1}^{j-1} (\\mathbf{u}_i^T \\mathbf{x}_j) \\mathbf{u}_i$$\n",
    "\n",
    "* Modified Algorithm\n",
    "\n",
    "$$\\mathbf{v}_j^{(k)} = \\mathbf{v}_j^{(k-1)} - (\\mathbf{u}_k^T \\mathbf{v}_j^{(k-1)}) \\mathbf{u}_k, \\quad k < j, \\quad \\mathbf{v}_j^{(0)} = \\mathbf{x}_j, \\quad \\mathbf{v}_j^{(j-1)} = \\mathbf{v}_j$$\n",
    "\n",
    "Let\n",
    "\n",
    "$$\\mathbf{\\hat{u}}_i = \\mathbf{u}_i + \\delta \\mathbf{u}_i$$\n",
    "\n",
    "for classical algorithm and\n",
    "\n",
    "$$\\mathbf{\\hat{u}}_k = \\mathbf{u}_k + \\delta \\mathbf{u}_k$$\n",
    "\n",
    "for modified algorithm\n",
    "\n",
    "**Classical:**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{\\hat{v}}_j &= \\mathbf{x}_j - \\sum_{i=1}^{j-1} (\\mathbf{u}_i + \\delta \\mathbf{u}_i)^T \\mathbf{x}_j (\\mathbf{u}_i + \\delta \\mathbf{u}_i) \\\\\n",
    "&= \\mathbf{x}_j - \\sum_{i=1}^{j-1} (\\mathbf{u}_i^T \\mathbf{x}_j) \\mathbf{u}_i - \\left[ \\sum_{i=1}^{j-1} (\\delta \\mathbf{u}_i^T \\mathbf{x}_j \\mathbf{u}_i) + \\sum_{i=1}^{j-1} (\\mathbf{u}_i^T \\mathbf{x}_j \\delta \\mathbf{u}_i) \\right] \\\\\n",
    "&= \\mathbf{v}_j + \\mathcal{O}(j)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Modified:** \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{\\hat{v}}_j^{(1)} &= \\mathbf{x}_j - (\\mathbf{u}_1 + \\delta \\mathbf{u}_1)^T \\mathbf{x}_j (\\mathbf{u}_1 + \\delta \\mathbf{u}_1) \\\\\n",
    "&= \\mathbf{x}_j - \\mathbf{u}_1^T \\mathbf{x}_j \\mathbf{u}_1 - (\\delta \\mathbf{u}_1^T \\mathbf{x}_j \\mathbf{u}_1 + \\mathbf{u}_1^T \\mathbf{x}_j \\delta \\mathbf{u}_1) \\\\\n",
    "&= \\mathbf{v}_j^{(1)} - \\delta \\mathbf{v}_j^{(1)} \\\\\n",
    "\\mathbf{\\hat{v}}_j^{(2)} &= \\mathbf{\\hat{v}}_j^{(1)} - (\\mathbf{u}_2 + \\delta \\mathbf{u}_2)^T \\mathbf{\\hat{v}}_j^{(1)} (\\mathbf{u}_2 + \\delta \\mathbf{u}_2) \\\\\n",
    "&= \\mathbf{\\hat{v}}_j^{(1)} - (\\mathbf{u}_2^T + \\delta \\mathbf{u}_2^T) (\\mathbf{v}_j^{(1)} - \\delta \\mathbf{v}_j^{(1)}) (\\mathbf{u}_2 + \\delta \\mathbf{u}_2) \\\\\n",
    "&= \\mathbf{v}_j^{(1)} - \\delta \\mathbf{v}_j^{(1)} - \\mathbf{u}_2^T \\mathbf{v}_j^{(1)} \\mathbf{u}_2 - (\\delta \\mathbf{u}_2^T \\mathbf{v}_j^{(1)} \\mathbf{u}_2 + \\mathbf{u}_2^T \\mathbf{v}_j^{(1)} \\delta \\mathbf{u}_2 - \\mathbf{u}_2^T \\delta \\mathbf{v}_j^{(1)} \\mathbf{u}_2)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{u}_2^T \\delta \\mathbf{v}_j^{(1)} \\mathbf{u}_2 = \\delta \\mathbf{v}_j^{(1)} \\mathbf{u}_2^T \\mathbf{u}_2 = \\delta \\mathbf{v}_j^{(1)}$, two terms canceled out, then:\n",
    "\n",
    "$$\\mathbf{\\hat{v}}_j^{(2)} = \\mathbf{v}_j^{(2)} - \\delta \\mathbf{v}_j^{(2)}$$\n",
    "\n",
    "Repeat this step and we will finally get:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{\\hat{v}}_j &= \\mathbf{v}_j - \\delta \\mathbf{v}_j^{(j-1)} \\\\\n",
    "&= \\mathbf{v}_j + \\mathcal{O}(1)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a544db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical: 181.20932543528096\n",
      "Modified: 5.962397721037823e-11\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import hilbert\n",
    "n = 200\n",
    "A = 0.00001* np.eye(n) + hilbert(n)\n",
    "Q1, R1 = qr_gram_schmidt(A)\n",
    "Q2, R2 = modified_qr(A)\n",
    "# Measure the loss of orthogonality\n",
    "print(f\"Classical: {np.linalg.norm(np.eye(n) - Q1.T @ Q1)}\")\n",
    "print(f\"Modified: {np.linalg.norm(np.eye(n) - Q2.T @ Q2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
