{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9be811",
   "metadata": {},
   "source": [
    "# Errors and Floating Point Numbers\n",
    "\n",
    "## Binary Numbers\n",
    "\n",
    "The computer stores the number in binary. Most people know how to express the integer part in binary number (For example, $(10)_{10}=(1\\times 2^3 + 0\\times 2^2 + 1\\times 2^1 + 0\\times 2^0)_{10}=(1010)_2$). It is the same method for expressing decimal numbers, just with negative exponents.\n",
    "\n",
    "For example, let's turn $(0.875)_{10}$ into binary number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f16950c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary number result is 0.111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = 0.875\n",
    "iters = 0\n",
    "bin = np.array([])\n",
    "while a != 0 and iters <= 16:\n",
    "    a = a * 2\n",
    "    iters += 1\n",
    "    if a < 1:\n",
    "        # Means that current binary bit is zero, it should be 0*2^-iters\n",
    "        bin = np.append(bin, 0)\n",
    "    else:\n",
    "        bin = np.append(bin, 1)\n",
    "        a -= 1\n",
    "\n",
    "bin = bin.astype(int)\n",
    "bin = bin.astype(str)\n",
    "bin = int(\"\".join(bin))\n",
    "print(f\"Binary number result is 0.{bin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa8e72b",
   "metadata": {},
   "source": [
    "We can find that $(0.875)_{10}=(1\\times 2^{-1}+1\\times 2^{-2}+1\\times 2^{-3})_{10}$\n",
    "\n",
    "However, some numbers cannot be expressed finitely in binary (such as 0.1 and 0.8). This is because the denominator of the simplest fraction is not in $2^n$. We can analogize it to a decimal: we get an infinite number of decimal places if the denominator is not a multiple of 2 or 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eda09d",
   "metadata": {},
   "source": [
    "## IEEE 754 Floating Point Standard\n",
    "Now, let's look deeper on how computer stores the number. IEEE 754 specifies how floating point numbers are rounded and stored (especially for the number that cannot be expressed finitely). It defines variour bits of floating numbers, from 16-bit to 256-bit. We represent binary numbers using a method similar to scientific notation:\n",
    "$$\\pm 1.bbb\\ldots b\\times 2^p$$\n",
    "Where $\\pm$ is the sign, $b$'s are the mantissa varies from 0 and 1, and $p$ is the exponent.\n",
    "\n",
    "For example, $(9)_{10}=(1001)_2=\\pm 1.001\\times 2^3$\n",
    "\n",
    "IEEE 754 standard for single (32-bit) and double (64-bit) precision floating point number is shown below:\n",
    "<center>\n",
    "\n",
    "| Format | Sign | Mantissa | Exponent |\n",
    "| :--- | :---: | :---: | :---: |\n",
    "| **Single (32-bit)** | 1 | 23 | 8 |\n",
    "| **Double (64-bit)** | 1 | 52 | 11 |\n",
    "\n",
    "</center>\n",
    "\n",
    "With normalized floating-point numbers, the precision will be super small when the number is small and super large when the number is significant. This is an acceptable trade-off between range and precision. While we are dealing with a large number, the required precision will not be very high. Hence, it is a reasonable choice to express the number in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c1548",
   "metadata": {},
   "source": [
    "## Machine Epsilon\n",
    "Definition of **Machine Epsilon**: $\\epsilon_{mach}$ is the distance between 1 and the smallest floating point number greater than 1. This is also related to the exponential term.\n",
    "\n",
    "E.g. $\\epsilon_{mach}$ for IEEE754 double precision number: \n",
    "$$\\epsilon_{mach}=1\\times 2^{-52}$$\n",
    "\n",
    "More specifically, this is called \"interval machine epsilon\". The \"rounding machine epsion\", which is half of it, will be used below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e668a475",
   "metadata": {},
   "source": [
    "## Chopping and Rounding\n",
    "**Chopping**: Throw away the bits beyond the last bit in the mantissa.\n",
    "\n",
    "**Rounding to Nearest**: If *n+1* bit is 0, truncate after the *n* bit (round down). If *n+1* bit is 1, usually we round up. If *n+1* bit is 1 and all other bits beyond are 0, then add 1 to the *n* bit if *n* bit is 1 (round up), otherwise round down.\n",
    "\n",
    "<center>\n",
    "\n",
    "| n+1 Bit | Bits n+2, n+3, ... | Current Bit n+1 | Final Action |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **0** | Any values | Any value | **Truncate** (Round Down) |\n",
    "| **1** | **At least one \"1\"** | Any value | **Add 1** (Round Up) |\n",
    "| **1** | **All \"0\"s** | **1** | **Add 1** (Round to Even) |\n",
    "| **1** | **All \"0\"s** | **0** | **Truncate** (Stay Even) |\n",
    "\n",
    "</center>\n",
    "\n",
    "***Note that IEEE 754 uses the rounding to nearest rule.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b3fd7",
   "metadata": {},
   "source": [
    "## Floating-Point Error\n",
    "\n",
    "Floating-point rounding satisfies:\n",
    "$$\\text{fl}(x)=x(1+\\delta),\\;|\\delta|\\le\\frac{\\epsilon_{mach}}{2}$$\n",
    "$$\\frac{|\\text{fl}(x)-x|}{|x|}\\le\\frac{\\epsilon_{mach}}{2}$$\n",
    "\n",
    "Here, the rounding machine epsilon is used.\n",
    "\n",
    "Floating-point arithmetics (like $+,-,\\times,\\div$) are subjected to the round to nearest rule. The error will accumulate and grow if we perform many such operations with a numerically unstable number. Usually $+$ and $\\times$ are well conditioned, $-$ is partially well conditioned, $\\div$ will be sometimes unstable.\n",
    "\n",
    "Here is the visualization of such error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e2fa52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  1.0000e+00  1.0000e+00  0.0000e+00    0.0000e+00    \n",
      "2  5.0000e-01  5.0000e-01  0.0000e+00    0.0000e+00    \n",
      "3  3.3333e-01  -2.1000e+01  2.1333e+01    6.4000e+01    \n",
      "4  2.5000e-01  2.5000e-01  0.0000e+00    0.0000e+00    \n",
      "5  2.0000e-01  6.5451e+06  6.5451e+06    3.2726e+07    \n",
      "6  1.6667e-01  -4.7664e+08  4.7664e+08    2.8599e+09    \n",
      "7  1.4286e-01  -9.8171e+09  9.8171e+09    6.8719e+10    \n",
      "8  1.2500e-01  1.2500e-01  0.0000e+00    0.0000e+00    \n",
      "9  1.1111e-01  4.9343e+12  4.9343e+12    4.4409e+13    \n",
      "10  1.0000e-01  1.4089e+14  1.4089e+14    1.4089e+15    \n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "K = 30\n",
    "n = []; err_abs = []; err_rel = []\n",
    "x = []; x_hat = []\n",
    "\n",
    "for i in range(N):\n",
    "    n.append(i + 1)\n",
    "    x.append(1 / n[i])\n",
    "    x_hat.append(x[i])\n",
    "    for j in range(K):\n",
    "        x_hat[i] = (n[i] + 1) * x_hat[i] - 1\n",
    "    err_abs.append(np.abs(x_hat[i] - x[i]))\n",
    "    err_rel.append(err_abs[i] / np.abs(x[i]))\n",
    "\n",
    "for i in range(N):\n",
    "    print(f\"{n[i]}  \"\n",
    "          f\"{x[i]:.4e}  \"\n",
    "          f\"{x_hat[i]:.4e}  \"\n",
    "          f\"{err_abs[i]:.4e}    \"\n",
    "          f\"{err_rel[i]:.4e}    \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98304dd6",
   "metadata": {},
   "source": [
    "## Forward and Backward Error\n",
    "\n",
    "The forward error is the output error, while the backward error is the input error.\n",
    "\n",
    "The absolute *output (forward)* error is:\n",
    "$$|\\delta y|=|\\hat{y}-y_{true}|=|\\hat{f}(x_{true})-f(x_{true})|$$\n",
    "\n",
    "The absolute *input (backward)* error is:\n",
    "$$|\\delta x|=|\\hat{x}-x_{true}|$$\n",
    "\n",
    "The forward and backward error can also be converted into each other:\n",
    "\n",
    "$$\\hat{y}=\\hat{f}(x)=f(\\hat{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2707b",
   "metadata": {},
   "source": [
    "## Conditioning\n",
    "Conditioning is a measurement on how sensitive the solution (or the error) is to small changes in the input. We can use an inequality to bound the error of the input and output.\n",
    "$$(\\text{relative output error})\\le\\kappa(\\text{relative input error})$$\n",
    "\n",
    "We can use a *condition number* to check if a problem is well conditioned or not:\n",
    "$$\\text{cond}=\\max_{\\text{input}}\\frac{\\text{relative output error}}{\\text{relative input error}}$$\n",
    "A problem is well conditioned if the condition number is small, vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de820e10",
   "metadata": {},
   "source": [
    "## Other Sources of Errors\n",
    "1. Numerical errors: While in numerical methods, we will sometimes calculate the integral and derivative numerically, which involves using the finite approximation method, and this will lead to error propagation.\n",
    "2. Modelling errors: Mathematical and physical models always contain some assumptions and approximations, which will introduce some errors. *\"All models are wrong, but some are useful.\"*\n",
    "3. Input errors: Parameters are never known exactly; we will introduce some errors while setting the parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
